{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9610d297",
   "metadata": {},
   "source": [
    "# Distributed Computing Inspired by Biology\n",
    "\n",
    "This notebook showcases the different bio-inspired algorithms discussed in\n",
    "> Függer, M., Nowak, T., Thuillier, K. (2025). Distributed Computing Inspired by Biology. Seminars in Cell and Developmental Biology.\n",
    "\n",
    "It requires the *Python 3* packages: `numpy`, `networkx`, `scipy`, `mobspy`, `ipykernel`, and `ipywidgets`.\n",
    "\n",
    "It can also be executed online (*without any installation*), using *Binder* [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/BioDisCo/dc_bio/HEAD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d08b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.notebook_utils import *  # noqa: F403\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e4a0a",
   "metadata": {},
   "source": [
    "## Hodkin-Huxley Neuron Model\n",
    "\n",
    "Mammalian brains contain a large number of neurons that interact to enable complex behavior.\n",
    "An early, simple model of a neuron is the Hodgkin-Huxley model introduced in\n",
    "> Alan L Hodgkin and Andrew F Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve.  \n",
    "> The Journal of physiology, 117(4):500, 1952\n",
    "\n",
    "It describes the kinetics of the membrane potential over time, for a given input current density.\n",
    "The model is in terms of a system of non-linear \\emph{ordinary differential equations} (ODEs) that describe the charging and discharging of a capacitance.\n",
    "An arguably simplistic model of a single neuron with an input current density $I(t)$ and a resulting membrane potential $V(t)$ is described by\n",
    "$$\n",
    "    \\begin{align*}\n",
    "    I(t) &= \\left \\{ \\begin{array}{l} \\text{threshold} \\times \\text{factor} \\quad \\text{if } 10.0 \\leq t \\\\ 0 \\quad \\text{else} \\end{array} \\right . \\\\\n",
    "    C \\cdot \\frac{dV(t)}{dt} &= I(t) - I_\\text{K}(V(t)) - I_\\text{Na}(V(t)) - I_\\text{L}(V(t)) \\quad \\quad \\text{with } V(t_0) = V_0\n",
    "    \\end{align*}\n",
    "$$\n",
    "as the charging of a capacitance $C$ by $I$.\n",
    "The non-linear discharging current densities of the sodium, potassium, and leak channels are described by\n",
    "$$\n",
    "\\begin{align*}\n",
    "    I_\\text{Na}(V) &= g_\\text{Na} \\cdot m^3(t) \\cdot h(t) \\cdot (V - V_\\text{Na})\\\\\n",
    "    I_\\text{K}(V) &= g_\\text{K} \\cdot n^4(t) \\cdot h(t) \\cdot (V - V_\\text{K})\\\\\n",
    "    I_\\text{L}(V) &= g_\\text{L} \\cdot (V - V_\\text{L})\n",
    "\\end{align*}\n",
    "$$\n",
    "where the dynamics of $m(t)$, $n(t)$, and $h(t)$ are defined by\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{dm(t)}{dt} &= \\alpha_m(V(t)) \\cdot (1 - m(t)) - \\beta_m(V(t)) \\cdot m(t) \\quad & \\text{with } m(t_0) = m_0 \\\\\n",
    "    \\frac{dn(t)}{dt} &= \\alpha_n(V(t)) \\cdot (1 - n(t)) - \\beta_n(V(t)) \\cdot n(t) \\quad & \\text{with } n(t_0) = n_0 \\\\\n",
    "    \\frac{dh(t)}{dt} &= \\alpha_h(V(t)) \\cdot (1 - h(t)) - \\beta_h(V(t)) \\cdot h(t) \\quad & \\text{with } h(t_0) = h_0\n",
    "\\end{align*}\n",
    "$$\n",
    "and $\\alpha$ and $\\beta$ are scaled and shifted exponential functions of $V(t)$.\n",
    "\n",
    "> See `src/hodgkin_huxley.py` for full implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208211aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_hodgkin_huxley(duration=200, steps=500)  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d035323",
   "metadata": {},
   "source": [
    "## Differentiable Loss Functions: Gradient Descent\n",
    "\n",
    "When the function to optimize is differentiable, gradient descent can be used to iteratively minimize it.\n",
    "In gradient descent, the parameters are updated in the direction opposite to the gradient of the loss function ($- \\frac{\\partial f(\\theta^{(i)})}{\\theta^{(i)}}$).  \n",
    "Let $0 < \\tau \\ll 1$ be the learning rate, let $f$ be the loss function, and let $\\theta^{(i)}$ be the parameters of $f$ at the $i$-th iteration.\n",
    "At each iteration $i$, the parameters $\\theta^{(i)}$ are updated as follows:\n",
    "$$\n",
    "    \\theta^{(i+1)} := \\theta^{(i)} - \\tau \\cdot \\frac{\\partial f(\\theta^{(i)})}{\\theta^{(i)}}\n",
    "$$\n",
    "\n",
    "In this notebook (as in the associated paper), we consider the differentiable function $f : \\mathbb{R}^2 \\to \\mathbb{R}$ defined by:\n",
    "$$\n",
    "    f(x, y) = \\sin(\\pi \\cdot x) \\cdot \\cos(\\pi \\cdot y) + (x^2 + y^2)\n",
    "$$\n",
    "whose derivatives are:\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\frac{\\partial f(x, y)}{\\partial x} &=  \\pi \\cdot \\cos(\\pi \\cdot x) \\cdot \\cos(\\pi \\cdot y) + 2 \\cdot x\n",
    "        \\quad\\quad\\quad\\quad\\quad \n",
    "        & \\frac{\\partial f(x, y)}{\\partial y} &= -\\pi \\cdot \\sin(\\pi \\cdot x) \\cdot \\sin(\\pi \\cdot y) + 2 \\cdot y\n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c00ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: float, y: float) -> float:  # noqa: D103\n",
    "    return np.sin(np.pi * x) * np.cos(np.pi * y) + (x**2 + y**2)\n",
    "\n",
    "\n",
    "def df_dx(x: float, y: float) -> float:  # noqa: D103\n",
    "    return np.pi * np.cos(np.pi * x) * np.cos(np.pi * y) + 2 * x\n",
    "\n",
    "\n",
    "def df_dy(x: float, y: float) -> float:  # noqa: D103\n",
    "    return -np.pi * np.sin(np.pi * x) * np.sin(np.pi * y) + 2 * y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430bd8d0",
   "metadata": {},
   "source": [
    "The paper discusses 3 variants of the gradient descent: *standard* gradient descent, a variant *with momentum*, and a variant *with noisy step*.\n",
    "For each method the iteration from $\\theta^{(i)}$ to $\\theta^{(i+1)}$ is given by:\n",
    "1. **Standard:** \n",
    "    * $\\theta^{(i+1)} := \\theta^{(i)} - \\tau \\cdot \\frac{\\partial f(\\theta^{(i)})}{\\theta^{(i)}}$\n",
    "\n",
    "2. **With momentum:**  \n",
    "    This variant introduces a velocity vector $\\nu_\\theta^{(i)}$ to incorporate past updates into the current step.\n",
    "    This helps accelerate convergence and may avoid local minima.  \n",
    "    Let $\\rho \\in [0, 1]$ be the momentum rate (referred to as `Momentum` below).  \n",
    "    Initialize $\\nu_\\theta^{(0)} = 0$, then update as:\n",
    "    * $\\nu^{(i+1)}_\\theta := \\rho \\cdot \\nu^{(i)}_{\\theta} - \\tau \\cdot \\frac{\\partial f(\\theta^{(i)})}{\\theta^{(i)}}$\n",
    "    * $\\theta^{(i+1)} := \\theta^{(i)} + \\nu^{(i+1)}_{\\theta}$\n",
    "\n",
    "3. **With noise:**  \n",
    "    This variant introduces Gaussian noise every $n$ steps to help escape local minima.  \n",
    "    Let $n \\in \\mathbb{N}$ be the number of iterations between random steps (referred to as `Noise` below), and let $X^{(i)} \\sim \\mathcal{N}(0, 1)$ be a standard Gaussian vector.  \n",
    "    At each iteration $i > 0$:\n",
    "    * $\\theta^{(i+1)} := \\theta^{(i)} - \\tau \\cdot \\begin{cases} X^{(i)} & \\text{if } i \\equiv 0 \\pmod{n} \\\\ \\frac{\\partial f(\\theta^{(i)})}{\\theta^{(i)}} & \\text{otherwise} \\end{cases}$\n",
    "\n",
    "> See `src/gradient_descent.py` for full implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1a1eb",
   "metadata": {},
   "source": [
    "In the cell below, the `Animation` checkbox can be used to display step-by-step the gradient descent algorithms.  \n",
    "⚠️ **Warning:** This animation may take some time to run. ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a8f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_gradient_descent(f, df_dx, df_dy)  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc6c551",
   "metadata": {},
   "source": [
    "## General Loss Functions\n",
    "\n",
    "Let's now consider the case when the function to optimize is not necessarily differentiable.\n",
    "\n",
    "We still consider the function $f : \\mathbb{R}^2 \\to \\mathbb{R}$ defined previously as:\n",
    "$$\n",
    "    f(x, y) = \\sin(\\pi \\cdot x) \\cdot \\cos(\\pi \\cdot y) + (x^2 + y^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bcd81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: float, y: float) -> float:  # noqa: D103\n",
    "    return np.sin(np.pi * x) * np.cos(np.pi * y) + (x**2 + y**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba9d68",
   "metadata": {},
   "source": [
    "### Differential Evolution\n",
    "\n",
    "Differential Evolution is an optimization method that iteratively estimates the optimum of a real-valued function $f$.  \n",
    "At each iteration, it generates new population of improving candidate solutions using three steps: *mutation* $\\rightarrow$ *recombination* $\\rightarrow$ *selection*.\n",
    "\n",
    "\n",
    "**Algorithm outlines**  \n",
    "\n",
    "* **Initialization:**  \n",
    "    Randomly sample $n \\in \\mathbb{N}^*$ points in the solution space. \n",
    "    Let $X^{(0)} = \\left \\{x^{(0)}_j \\right \\}_{j = 1}^n$ be our initial population.\n",
    "    \n",
    "* **Iteration:**  \n",
    "    For each iteration $i \\geq 0$, the next population $X^{(i+1)} = \\left \\{ x_j^{(i+1)} \\right \\}_{j=1}^{n}$ is generated from $X^{(i)}$ as follows:\n",
    "    1. **Mutation:**  \n",
    "        For each individual $x_j^{(i)}$, randomly select three distinct individuals $x_{r1}^{(i)}$, $x_{r2}^{(i)}$, and $x_{r3}^{(i)}$ from the population.\n",
    "        Construct a mutant vector:\n",
    "        $$\n",
    "            v^{(i+1)}_j = x^{(i)}_{r1} + \\mu \\cdot (x^{(i)}_{r2} - x^{(i)}_{r3})\n",
    "        $$\n",
    "        where $\\mu \\in \\mathbb{R}^+$ is the mutation rate (referred to as `Mutation` below).\n",
    "    \n",
    "    2. **Recombination:**  \n",
    "        Create a candidate vector by mixing components of $x_j^{(i)}$ and $v_j^{(i+1)}$.\n",
    "        For each component $k$:\n",
    "        $$\n",
    "            v^{(i+1)}_{j,k} := \\begin{cases} v^{(i+1)}_{j,k} & \\text{with probability } \\phi \\\\ x^{(i)}_{j,k} & \\text{with probability } 1 - \\phi \\end{cases}\n",
    "        $$\n",
    "        where $\\phi \\in [0, 1]$ is the recombination rate (referred to as `Recombination` belows).\n",
    "\n",
    "    3. **Selection:**  \n",
    "        Keep the candidate individual that yields the lowest loss *w.r.t.* $f$:\n",
    "        $$\n",
    "            x^{(i+1)}_{j} = \\begin{cases} v^{(i+1)}_{j} & \\text{if } f(v^{(i+1)}_{j}) \\leq f(x^{(i)}_{j}) \\\\ x^{(i)}_{j} & \\text{otherwise} \\end{cases}\n",
    "        $$\n",
    "\n",
    "> See `src/differential_evolution.py` for full implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a73a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_differential_evolution(f)  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd7817",
   "metadata": {},
   "source": [
    "### Particle Swarm\n",
    "\n",
    "Particle Swarm Optimization is a stochastic optimization method.\n",
    "It is used to find the global minimum (or maximum) of a real-valued function over a continuous space.\n",
    "\n",
    "Here, each candidate solution is a particle moving through the search space.\n",
    "At each iteration, particles adjust their position based on their momentum, the best value they found, and the best value found by all neighboring particles.\n",
    "\n",
    "\n",
    "**Algorithm outlines**  \n",
    "Given $n$ the number of particles, we denote by $x^{(i)}_j$ and $\\nu^{(i)}_j$ the position and velocity of particle $j$, *respectively*, at the $i$-th iteration.\n",
    "We also denote by $p^{(i)}_j$ the best position found by particle $j$ so far (***personal best***), and by $g$ the best position found by the entire swarm (***global best***).\n",
    "\n",
    "* **Initialization:**  \n",
    "    For all particles $j$: $x^{(0)}_j$ is randomly chosen in the solution space; $\\nu^{(0)}_j = 0$; and $p^{(0)}_j = x^{(0)}_j$.  \n",
    "    The global best value $g$ is initialized as $g = p^{(0)}_{\\argmax_j f(p^{(0)}_j)}$.\n",
    "\n",
    "* **Iteration:**  \n",
    "    For all particles $j$:\n",
    "    - Position and velocity updates *w.r.t.* personal and global bests.\n",
    "        $$\n",
    "            \\begin{align*}\n",
    "                \\nu^{(i+1)}_j &= \\rho \\cdot \\nu^{(i)}_j + w_p \\cdot X^{(i)}_{j} \\cdot (p^{(i)}_j - x^{(i)}_j)  + w_g \\cdot Y^{(i)}_{j} \\cdot (g - x^{(i)}_j) \\\\\n",
    "                x^{(i+1)}_j &= x^{(i)}_j + \\nu^{(i+1)}_j\n",
    "            \\end{align*}\n",
    "        $$\n",
    "        where $X^{(i)}_{j}, Y^{(i)}_{j} \\sim \\mathcal{N}(0, 1)$ are two random vectors, $\\rho$ is the momentum rate, and $w_p, w_g \\in \\mathbb{R}^+$ are the attraction strengths toward the personal and global bests, *respectively*.\n",
    "\n",
    "    - Personal and global best updates.\n",
    "        $$\n",
    "            p^{(i+1)}_j := \\begin{cases} x^{(i+1)}_{j} & \\text{if } f(x^{(i+1)}_{j}) \\leq f(p^{(i)}_{j}) \\\\ p^{(i)}_{j} & \\text{otherwise} \\end{cases}\n",
    "            \\quad \\quad \\quad \\quad\n",
    "            g := \\begin{cases} p^{(i+1)}_{j} & \\text{if } f(p^{(i+1)}_{j}) \\leq f(g) \\\\ g & \\text{otherwise} \\end{cases} \\\\\n",
    "        $$\n",
    "\n",
    "\n",
    "> See `src/particle_swarm.py` for full implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_particle_swarm(f)  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a546cdc",
   "metadata": {},
   "source": [
    "## Consensus\n",
    "\n",
    "### Graphs Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c4872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs, sequence = interact_consensus_graphs()  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8bca40",
   "metadata": {},
   "source": [
    "### 1D Methods\n",
    "\n",
    "#### Equal-weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_consensus_1D(sequence, \"mean\")  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dcf56a",
   "metadata": {},
   "source": [
    "#### Midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_consensus_1D(sequence, \"midpoint\")  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5abc1ae",
   "metadata": {},
   "source": [
    "### 2D Methods: MidExtreme vs ApproachExtreme\n",
    "\n",
    "#### MidExtreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_consensus_2D(sequence, \"midextreme\")  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a8c58",
   "metadata": {},
   "source": [
    "#### ApproachExtreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_consensus_2D(sequence, \"approachextreme\")  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f203a129",
   "metadata": {},
   "source": [
    "## Chemical Reaction Networks\n",
    "\n",
    "### ABC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd03c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_crn_abc(duration=10)  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f4080",
   "metadata": {},
   "source": [
    "### Annihilation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6bd6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_crn_mutual_annihilation(duration=10)  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70540911",
   "metadata": {},
   "source": [
    "## Maximal Independent Set - MIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae8818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_maximal_independent_set()  # noqa: F405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa3c20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
