{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d90f32e",
   "metadata": {},
   "source": [
    "# Distributed Computing Inspired by Biology\n",
    "\n",
    "This notebook showcases the different distributed algorithms discussed in\n",
    "> Függer, M., Nowak, T., Thuillier, K. (2025). Distributed Computing Inspired by Biology. Seminars in Cell and Developmental Biology.\n",
    "\n",
    "It requires the *Python 3* packages: `numpy`, `networkx`, `scipy`, `mobspy`, `ipykernel`, and `ipywidgets`.\n",
    "A virtual *Python 3* environment with all dependencies can be installed with:\n",
    "```bash\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "It can also be executed online (*without any installation*), using *Binder* [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/BioDisCo/dc_bio/HEAD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac7342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import cos, pi, sin\n",
    "\n",
    "from src.notebook_utils import *  # noqa: F403\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b500f5",
   "metadata": {},
   "source": [
    "In the cells below, the `Animation` checkboxes can be used to display step-by-step the iterations of the different algorithms.  \n",
    "⚠️ **Warning:** Those animations may take some time to run. ⚠️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c386fa8c",
   "metadata": {},
   "source": [
    "## Convergence of Agent States\n",
    "\n",
    "This section presents the consensus algorithms discussed in the paper.\n",
    "Agents and their communications are typically modeled using *communication graphs*, *i.e.*, directed graphs.\n",
    "A node is an agent, and an edge is a potential communication between two agents.\n",
    "In practice, communication graphs are often dynamic, changing over time.\n",
    "\n",
    "Given an initial system state where each agent holds its own local value, consensus algorithms enable agents to iteratively update their states to eventually agree on a common value.  \n",
    "To simplify notation, let's assume that the local value of agent $i$ is a set $X_i = \\{ x_1, x_2, \\cdots\\}, \\quad x_j \\in \\mathbb{R}^n$.\n",
    "We also denote by $\\delta^+_{G_k}(i)$ the set of neighboors of agent $i$ in the communication graph $G_k$\n",
    "\n",
    "In this notebook (as in the paper), we consider two types of iterations:\n",
    "\n",
    "1. *standard iteration*:  \n",
    "    Each node/agent updates its local value according to its neighboors' (in the current communication graph) local values.  \n",
    "    The local value of agent $i$ is updated *w.r.t.* an update function $f$ such that: \n",
    "    $$\n",
    "        X_i := f(X_i \\cup \\bigcup_{j \\in \\delta^+_{G_k}(i)}  X_j)\n",
    "    $$\n",
    "    \n",
    "2. *flooding iteration*:  \n",
    "    Each node/agent sends its local value to its neighbors.  \n",
    "    The local value of agent $i$ is updated such that: \n",
    "    $$\n",
    "        X_i := X_i \\cup \\bigcup_{j \\in \\delta^+_{G_k}(i)}  X_j\n",
    "    $$\n",
    "\n",
    "\n",
    "**Outlines.** In the next subsections, we first generate the communication graphs sequence.\n",
    "Then, we describe four algorithms / update functions: *equal-weight* (1D), *midpoints* (1D), *midextremes* (2D), and *approachextremes* (2D).\n",
    "\n",
    "For each algorithm, we compare the convergence with and without *flooding iterations*.\n",
    "In the following interactive cells, `flooding step` denotes the number of *flooding iterations* between two *standard iterations*.\n",
    "Initial local values are randomized.\n",
    "\n",
    "> See `src/consensus.py` for full implementation details.\n",
    "\n",
    "\n",
    "### Graph Generation\n",
    "\n",
    "We begin by generating a sequence of communication graphs for the consensus algorithms.  \n",
    "To do so, we randomly generate $m$ rooted directed graphs with $n$ nodes.\n",
    "Let $\\mathcal{G} = \\{ G_i \\}_{i=1}^{m}$ denote this set of random rooted directed graphs.  \n",
    "The communication graph sequence is then constructed by randomly sampling $l$ graphs from $\\mathcal{G}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs, sequence = interact_consensus_graphs()  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d942f4",
   "metadata": {},
   "source": [
    "### 1D Methods\n",
    "\n",
    "This subsection is dedicated to one dimensional local values.\n",
    "\n",
    "#### Equal-weight\n",
    "\n",
    "Given $X = \\{ x_j \\}_j$ for $x_j \\in \\mathbb{R}$, the update function is the mean of all values:\n",
    "$$\n",
    "    f_\\text{equal-weight}(X) = \\left \\{ \\frac{\\sum_j x_j }{|X|} \\right \\}\n",
    "$$\n",
    "\n",
    "The interactive cell belows show the local value of each node for each iteration of the consensus algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f77632",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_consensus_1D(sequence, \"mean\")  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e87dee2",
   "metadata": {},
   "source": [
    "#### Midpoint\n",
    "\n",
    "Given $X = \\{ x_j \\}_j$ for $x_j \\in \\mathbb{R}$, the update function is:\n",
    "$$\n",
    "    f_\\text{midpoint}(X) = \\left \\{ \\frac{\\max(X) - \\min(X) }{2} \\right \\}\n",
    "$$\n",
    "\n",
    "The interactive cell belows show the local value of each node for each iteration of the consensus algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff1c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_consensus_1D(sequence, \"midpoint\")  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef287138",
   "metadata": {},
   "source": [
    "### 2D Methods: MidExtreme vs ApproachExtreme\n",
    "\n",
    "This subsection is dedicated to $n$-dimensional local values.\n",
    "For the sake of our example, $n = 2$.\n",
    "\n",
    "#### MidExtreme\n",
    "\n",
    "Given $X = \\{ x_j \\}_j$ for $x_j \\in \\mathbb{R}^n$, the update function is:\n",
    "$$\n",
    "    f_\\text{midextreme}(X) = \\left \\{ \\frac{\\argmax_{(x, y) \\in X \\times X} ||x - y||_2 }{2} \\right \\}\n",
    "$$\n",
    "This function returns a new set of local values containing exactly one point: the midpoint between the two most distant points among all values currently held by the agent and its neighbors.\n",
    "\n",
    "The interactive cell belows show the local value of each node for each iteration of the consensus algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_consensus_2D(sequence, \"midextreme\")  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0b8b06",
   "metadata": {},
   "source": [
    "#### ApproachExtreme\n",
    "\n",
    "Given $X = \\{ x_j \\}_j$ for $x_j \\in \\mathbb{R}^n$ and $x \\in \\mathbb{R}^n$ the local value of the agent to update, the update function is:\n",
    "$$\n",
    "    f_\\text{approachextreme}(x, X) = \\left \\{ \\frac{\\argmax_{y \\in X} ||x - y||_2 }{2} \\right \\}\n",
    "$$\n",
    "Unlike for *midextreme*, this function returns a new set of local values containing exactly one point: the midpoint between the current agent's local value and the point most distant from it among all values held by its neighbors and values received during flooding.\n",
    "\n",
    "The interactive cell belows show the local value of each node for each iteration of the consensus algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_consensus_2D(sequence, \"approachextreme\")  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf912555",
   "metadata": {},
   "source": [
    "## Optimization by Agents\n",
    "\n",
    "This section presents the optimization methods discussed in the paper.  \n",
    "\n",
    "### Graph optimization: Maximal Independent Set\n",
    "\n",
    "Given an undirected graph $G = (V, E)$ where $V$ is the set of vertices and $E \\subseteq V \\times V$ is the set of edges, a *maximal independent set* (MIS), denoted by $\\text{MIS}(G)$ is a subset of $V$ such that:\n",
    "1. no two nodes that are neighbors in the graph are in the MIS, *i.e.*, $\\forall (u, v) \\in E, \\left(u \\not \\in \\text{MIS}(G) \\right) \\lor \\left(v \\not \\in \\text{MIS}(G) \\right)$;\n",
    "2. the MIS is superset maximal, *i.e.*, no node can be added to the set without invalidating the first property.\n",
    "\n",
    "Inspired from fly's cells communications to form sensory bristles, a bio-inspired MIS algorithm has been developed.  \n",
    "> Yehuda Afek, Noga Alon, Omer Barad, Eran Hornstein, Naama Barkai, and Ziv Bar-Joseph. A Biological Solution to a Fundamental Distributed Computing Problem.  \n",
    "> Science, 331(6014):183–185, January 2011. Publisher: American Association for the Advancement of Science.\n",
    "\n",
    "This iterative algorithm works as follow.\n",
    "At each iteration:\n",
    "\n",
    "1. Each agent broadcasts a single bit message to their neighbors with a probability $p$ or remain silent. The probability $p$ increase through time.  \n",
    "    For our implementation, we use $p = \\frac{1}{2}^{\\log{C} - i}$ with $C = \\max_{v \\in V} |\\{u | \\u \\in V \\land (u, v) \\in E \\}|$ (*i.e.*, maximum degree in the graph) and $i$ the current iteration, as proposed in the aformentionned paper.\n",
    "\n",
    "2. An agent that has sent a message and does not receive any joins the MIS.\n",
    "\n",
    "It has been proved (in the aformentionned paper) that after a sufficiently large number of iterations, the algorithm generate a MIS with high probability.\n",
    "Note that it always ensures that no two neighboring nodes join the set.\n",
    "In practice, we fixe the number of iteration at $20 \\times \\lceil \\log(C) \\rceil$ as proposed in the aformentionned paper.\n",
    "\n",
    "In the cell below, the MIS algorithm is applied to a lattice graph.\n",
    "\n",
    "> See `src/maximal_independent_set.py` for full implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4408427",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_maximal_independent_set()  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4862944",
   "metadata": {},
   "source": [
    "### General Loss Functions\n",
    "\n",
    "Let's consider the general case where we want to optimize a given function $f$.\n",
    "\n",
    "For our examples, we consider the function $f : \\mathbb{R}^2 \\to \\mathbb{R}$ defined as:\n",
    "$$\n",
    "    f(x, y) = \\sin(\\pi \\cdot x) \\cdot \\cos(\\pi \\cdot y) + (x^2 + y^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: float, y: float) -> float:  # noqa: D103\n",
    "    return sin(pi * x) * cos(pi * y) + (x**2 + y**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e87656",
   "metadata": {},
   "source": [
    "#### Differential Evolution\n",
    "\n",
    "Differential Evolution is an optimization method that iteratively estimates the optimum of a real-valued function $f$.  \n",
    "At each iteration, it generates new population of improving candidate solutions using three steps: *mutation* $\\rightarrow$ *recombination* $\\rightarrow$ *selection*.\n",
    "\n",
    "\n",
    "**Algorithm outlines**  \n",
    "\n",
    "* **Initialization:**  \n",
    "    Randomly sample $n \\in \\mathbb{N}^*$ points in the solution space (referred to as `Population` below). \n",
    "    Let $X^{(0)} = \\left \\{x^{(0)}_j \\right \\}_{j = 1}^n$ be our initial population.\n",
    "    \n",
    "* **Iteration:**  \n",
    "    For each iteration $i \\geq 0$, the next population $X^{(i+1)} = \\left \\{ x_j^{(i+1)} \\right \\}_{j=1}^{n}$ is generated from $X^{(i)}$ as follows:\n",
    "    1. **Mutation:**  \n",
    "        For each individual $x_j^{(i)}$, randomly select three distinct individuals $x_{r1}^{(i)}$, $x_{r2}^{(i)}$, and $x_{r3}^{(i)}$ from the population.\n",
    "        Construct a mutant vector:\n",
    "        $$\n",
    "            v^{(i+1)}_j = x^{(i)}_{r1} + \\mu \\cdot (x^{(i)}_{r2} - x^{(i)}_{r3})\n",
    "        $$\n",
    "        where $\\mu \\in \\mathbb{R}^+$ is the mutation rate (referred to as `Mutation` below).\n",
    "    \n",
    "    2. **Recombination:**  \n",
    "        Create a candidate vector by mixing components of $x_j^{(i)}$ and $v_j^{(i+1)}$.\n",
    "        For each component $k$:\n",
    "        $$\n",
    "            v^{(i+1)}_{j,k} := \\begin{cases} v^{(i+1)}_{j,k} & \\text{with probability } \\phi \\\\ x^{(i)}_{j,k} & \\text{with probability } 1 - \\phi \\end{cases}\n",
    "        $$\n",
    "        where $\\phi \\in [0, 1]$ is the recombination rate (referred to as `Recombination` belows).\n",
    "\n",
    "    3. **Selection:**  \n",
    "        Keep the candidate individual that yields the lowest loss *w.r.t.* $f$:\n",
    "        $$\n",
    "            x^{(i+1)}_{j} = \\begin{cases} v^{(i+1)}_{j} & \\text{if } f(v^{(i+1)}_{j}) \\leq f(x^{(i)}_{j}) \\\\ x^{(i)}_{j} & \\text{otherwise} \\end{cases}\n",
    "        $$\n",
    "\n",
    "> See `src/differential_evolution.py` for full implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33f52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_differential_evolution(f)  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd06649",
   "metadata": {},
   "source": [
    "#### Particle Swarm Optimization\n",
    "\n",
    "Particle Swarm Optimization is a stochastic optimization method.\n",
    "It is used to find the global minimum (or maximum) of a real-valued function over a continuous space.\n",
    "\n",
    "Here, each candidate solution is a particle moving through the search space.\n",
    "At each iteration, particles adjust their position based on their momentum, the best value they found, and the best value found by all neighboring particles.\n",
    "\n",
    "\n",
    "**Algorithm outlines**  \n",
    "Given $n$ the number of particles, we denote by $x^{(i)}_j$ and $\\nu^{(i)}_j$ the position and velocity of particle $j$, *respectively*, at the $i$-th iteration.\n",
    "We also denote by $p^{(i)}_j$ the best position found by particle $j$ so far (***personal best***), and by $g$ the best position found by the entire swarm (***global best***).\n",
    "\n",
    "* **Initialization:**  \n",
    "    For all particles $j$: $x^{(0)}_j$ is randomly chosen in the solution space; $\\nu^{(0)}_j = 0$; and $p^{(0)}_j = x^{(0)}_j$.  \n",
    "    The global best value $g$ is initialized as $g = p^{(0)}_{\\argmax_j f(p^{(0)}_j)}$.\n",
    "\n",
    "* **Iteration:**  \n",
    "    For all particles $j$:\n",
    "    - Position and velocity updates *w.r.t.* personal and global bests.\n",
    "        $$\n",
    "            \\begin{align*}\n",
    "                \\nu^{(i+1)}_j &= \\rho \\cdot \\nu^{(i)}_j + w_p \\cdot X^{(i)}_{j} \\cdot (p^{(i)}_j - x^{(i)}_j)  + w_g \\cdot Y^{(i)}_{j} \\cdot (g - x^{(i)}_j) \\\\\n",
    "                x^{(i+1)}_j &= x^{(i)}_j + \\nu^{(i+1)}_j\n",
    "            \\end{align*}\n",
    "        $$\n",
    "        where $X^{(i)}_{j}, Y^{(i)}_{j} \\sim \\mathcal{N}(0, 1)$ are two random vectors, $\\rho$ is the momentum rate, and $w_p, w_g \\in \\mathbb{R}^+$ are the attraction strengths toward the personal and global bests, *respectively*.\n",
    "\n",
    "    - Personal and global best updates.\n",
    "        $$\n",
    "            p^{(i+1)}_j := \\begin{cases} x^{(i+1)}_{j} & \\text{if } f(x^{(i+1)}_{j}) \\leq f(p^{(i)}_{j}) \\\\ p^{(i)}_{j} & \\text{otherwise} \\end{cases}\n",
    "            \\quad \\quad \\quad \\quad\n",
    "            g := \\begin{cases} p^{(i+1)}_{j} & \\text{if } f(p^{(i+1)}_{j}) \\leq f(g) \\\\ g & \\text{otherwise} \\end{cases} \\\\\n",
    "        $$\n",
    "\n",
    "\n",
    "> See `src/particle_swarm.py` for full implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_particle_swarm(f)  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f9310",
   "metadata": {},
   "source": [
    "### Differentiable Loss Functions: Gradient Descent\n",
    "\n",
    "When the function to optimize is differentiable, gradient descent can be used to iteratively minimize it.\n",
    "In gradient descent, the parameters are updated in the direction opposite to the gradient of the loss function ($- \\frac{\\partial f(\\theta^{(i)})}{\\theta^{(i)}}$).  \n",
    "Let $0 < \\tau \\ll 1$ be the learning rate, let $f$ be the loss function, and let $\\theta^{(i)}$ be the parameters of $f$ at the $i$-th iteration.\n",
    "At each iteration $i$, the parameters $\\theta^{(i)}$ are updated as follows:\n",
    "$$\n",
    "    \\theta^{(i+1)} := \\theta^{(i)} - \\tau \\cdot \\frac{\\partial f(\\theta^{(i)})}{\\theta^{(i)}}\n",
    "$$\n",
    "\n",
    "For the examples, we consider the differentiable function $f : \\mathbb{R}^2 \\to \\mathbb{R}$ defined by:\n",
    "$$\n",
    "    f(x, y) = \\sin(\\pi \\cdot x) \\cdot \\cos(\\pi \\cdot y) + (x^2 + y^2)\n",
    "$$\n",
    "whose derivatives are:\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\frac{\\partial f(x, y)}{\\partial x} &=  \\pi \\cdot \\cos(\\pi \\cdot x) \\cdot \\cos(\\pi \\cdot y) + 2 \\cdot x\n",
    "        \\quad\\quad\\quad\\quad\\quad \n",
    "        & \\frac{\\partial f(x, y)}{\\partial y} &= -\\pi \\cdot \\sin(\\pi \\cdot x) \\cdot \\sin(\\pi \\cdot y) + 2 \\cdot y\n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: float, y: float) -> float:  # noqa: D103\n",
    "    return sin(pi * x) * cos(pi * y) + (x**2 + y**2)\n",
    "\n",
    "\n",
    "def df_dx(x: float, y: float) -> float:  # noqa: D103\n",
    "    return pi * cos(pi * x) * cos(pi * y) + 2 * x\n",
    "\n",
    "\n",
    "def df_dy(x: float, y: float) -> float:  # noqa: D103\n",
    "    return -pi * sin(pi * x) * sin(pi * y) + 2 * y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ecfdd1",
   "metadata": {},
   "source": [
    "The paper discusses 3 variants of the gradient descent: *standard* gradient descent, a variant *with momentum*, and a variant *with noisy step*.  \n",
    "For each method the iteration from $\\theta^{(i)}$ to $\\theta^{(i+1)}$ is given by:\n",
    "1. **Standard:** \n",
    "    * $\\theta^{(i+1)} := \\theta^{(i)} - \\tau \\cdot \\frac{\\partial f(\\theta^{(i)})}{\\theta^{(i)}}$\n",
    "\n",
    "2. **With momentum:**  \n",
    "    This variant introduces a velocity vector $\\nu_\\theta^{(i)}$ to incorporate past updates into the current step.\n",
    "    Let $\\rho \\in [0, 1]$ be the momentum rate (referred to as `Momentum` below).  \n",
    "    Initialize $\\nu_\\theta^{(0)} = 0$, then update as:\n",
    "    * $\\nu^{(i+1)}_\\theta := \\rho \\cdot \\nu^{(i)}_{\\theta} - \\tau \\cdot \\frac{\\partial f(\\theta^{(i)})}{\\theta^{(i)}}$\n",
    "    * $\\theta^{(i+1)} := \\theta^{(i)} + \\nu^{(i+1)}_{\\theta}$\n",
    "\n",
    "3. **With noise:**  \n",
    "    This variant introduces Gaussian noise every $n$ steps to help escape local minima.  \n",
    "    Let $n \\in \\mathbb{N}$ be the number of iterations between random steps (referred to as `Noise` below), and let $X^{(i)} \\sim \\mathcal{N}(0, 1)$ be a standard Gaussian vector.  \n",
    "    At each iteration $i > 0$:\n",
    "    * $\\theta^{(i+1)} := \\theta^{(i)} - \\tau \\cdot \\begin{cases} X^{(i)} & \\text{if } i \\equiv 0 \\pmod{n} \\\\ \\frac{\\partial f(\\theta^{(i)})}{\\theta^{(i)}} & \\text{otherwise} \\end{cases}$\n",
    "\n",
    "> See `src/gradient_descent.py` for full implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa23da",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_gradient_descent(f, df_dx, df_dy)  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5006549",
   "metadata": {},
   "source": [
    "## Competing Neuron Models: the Hodgkin-Huxley Model\n",
    "\n",
    "Mammalian brains contain a large number of neurons that interact to enable complex behavior.\n",
    "An early, simple model of a neuron is the Hodgkin-Huxley model introduced in\n",
    "> Alan L Hodgkin and Andrew F Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve.  \n",
    "> The Journal of physiology, 117(4):500, 1952\n",
    "\n",
    "It describes the kinetics of the membrane potential over time, for a given input current density.\n",
    "The model is in terms of a system of non-linear \\emph{ordinary differential equations} (ODEs) that describe the charging and discharging of a capacitance.\n",
    "An arguably simplistic model of a single neuron with an input current density $I(t)$ and a resulting membrane potential $V(t)$ is described by\n",
    "$$\n",
    "    \\begin{align*}\n",
    "    I(t) &= \\left \\{ \\begin{array}{l} \\text{threshold} \\times \\text{factor} \\quad \\text{if } 10.0 \\leq t \\\\ 0 \\quad \\text{else} \\end{array} \\right . \\\\\n",
    "    C \\cdot \\frac{dV(t)}{dt} &= I(t) - I_\\text{K}(V(t)) - I_\\text{Na}(V(t)) - I_\\text{L}(V(t)) \\quad \\quad \\text{with } V(t_0) = V_0\n",
    "    \\end{align*}\n",
    "$$\n",
    "as the charging of a capacitance $C$ by $I$.\n",
    "The non-linear discharging current densities of the sodium, potassium, and leak channels are described by\n",
    "$$\n",
    "\\begin{align*}\n",
    "    I_\\text{Na}(V) &= g_\\text{Na} \\cdot m^3(t) \\cdot h(t) \\cdot (V - V_\\text{Na})\\\\\n",
    "    I_\\text{K}(V) &= g_\\text{K} \\cdot n^4(t) \\cdot h(t) \\cdot (V - V_\\text{K})\\\\\n",
    "    I_\\text{L}(V) &= g_\\text{L} \\cdot (V - V_\\text{L})\n",
    "\\end{align*}\n",
    "$$\n",
    "where the dynamics of $m(t)$, $n(t)$, and $h(t)$ are defined by\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{dm(t)}{dt} &= \\alpha_m(V(t)) \\cdot (1 - m(t)) - \\beta_m(V(t)) \\cdot m(t) \\quad & \\text{with } m(t_0) = m_0 \\\\\n",
    "    \\frac{dn(t)}{dt} &= \\alpha_n(V(t)) \\cdot (1 - n(t)) - \\beta_n(V(t)) \\cdot n(t) \\quad & \\text{with } n(t_0) = n_0 \\\\\n",
    "    \\frac{dh(t)}{dt} &= \\alpha_h(V(t)) \\cdot (1 - h(t)) - \\beta_h(V(t)) \\cdot h(t) \\quad & \\text{with } h(t_0) = h_0\n",
    "\\end{align*}\n",
    "$$\n",
    "and $\\alpha$ and $\\beta$ are scaled and shifted exponential functions of $V(t)$.\n",
    "\n",
    "> See `src/hodgkin_huxley.py` for full implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09073bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_hodgkin_huxley(duration=200, steps=500)  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f0b43",
   "metadata": {},
   "source": [
    "## Biology Inspired by Distributed Computing -- Chemical Reaction Networks\n",
    "\n",
    "Inspired by distributed computing, researchers are recently starting to implement distributed algorithms into biological systems.\n",
    "Chemical reaction networks are a mathematical model for reaction kinetics.\n",
    "In particular, it is used to model the dynamics of biological systems. \n",
    "\n",
    "This section shows the simulations, deterministic and stochastic, of two chemical reaction networks: a simple model `ABC`, and a model of *mutual annihilation* `MutualAnnihilation` inspired from distributed algorithms.  \n",
    "\n",
    "> See `src/chemical_reaction_network.py` for full implementation details.\n",
    "\n",
    "### ABC Model\n",
    "\n",
    "This model is composed of 3 biological entities `A`, `B`, and `C`, and 2 reactions:\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        A + B &\\rightarrow A + C &\\quad[k_1]\\\\\n",
    "        A &\\rightarrow \\emptyset &\\quad [k_2]\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "The associated ODE system is:\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\frac{\\partial A(t)}{\\partial t} &= -k_2 \\cdot A(t)\\\\\n",
    "        \\frac{\\partial B(t)}{\\partial t} &= -k_1 \\cdot A(t) \\cdot B(t)\\\\\n",
    "        \\frac{\\partial C(t)}{\\partial t} &=  k_1 \\cdot A(t) \\cdot B(t)\n",
    "    \\end{align*}\n",
    "$$\n",
    "with $A(t_0) = A_0$, $B(t_0) = B_0$, and $C(t_0) = C_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_crn_abc(duration=10)  # noqa: F405"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657ad7a",
   "metadata": {},
   "source": [
    "### Mutual Annihilation Model\n",
    "\n",
    "This model is composed of 3 biological entities `A`, `B`, and `R` (for the available ressources), and 4 reactions:\n",
    "$$\n",
    "    \\begin{align*}\n",
    "    A + B &\\rightarrow A &\\quad[k_1]\\\\\n",
    "    A + B &\\rightarrow B &\\quad[k_2]\\\\\n",
    "    A + R &\\rightarrow A + A &\\quad [k_3]\\\\\n",
    "    B + R &\\rightarrow B + B &\\quad [k_4]\n",
    "    \\end{align*}\n",
    "$$\n",
    "This biological model is inspired by distributed algorithms, namely *voting algorithms*.\n",
    "At the end of the simulation, only the entity that was initially in the majority (`A` or `B`) remains.\n",
    "\n",
    "The associated ODE system is:\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\frac{\\partial A(t)}{\\partial t} &= -k_2 \\cdot A(t) \\cdot B(t) + k_3 \\cdot A(t) \\cdot R(t) \\\\\n",
    "        \\frac{\\partial B(t)}{\\partial t} &= -k_1 \\cdot A(t) \\cdot B(t) + k_4 \\cdot B(t) \\cdot R(t) \\\\\n",
    "        \\frac{\\partial R(t)}{\\partial t} &= -k_3 \\cdot A(t) \\cdot R(t) -k_4 \\cdot B(t) \\cdot R(t)\n",
    "    \\end{align*}\n",
    "$$\n",
    "with $A(t_0) = A_0$, $B(t_0) = B_0$, and $R(t_0) =R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46111f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_crn_mutual_annihilation(duration=10)  # noqa: F405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286c779b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
